[job]
dump_folder = "./outputs"
description = "Mamba Debugging"
config_file = "configs/mamba_debug.toml"
validate_job = false
print_args = false

[profiling]
enable_profiling = true
save_traces_folder = "profile_trace"
profile_freq = 100
enable_memory_snapshot = false
save_memory_snapshot_folder = "memory_snapshot"

[metrics]
disable_color_printing = false
log_freq = 10
enable_tensorboard = true
save_tb_folder = "tb"
enable_wandb = false
rank_0_only = true

[model]
name = "mamba"
variant = "debug"
norm_type = "rmsnorm"  # layernorm / np_layernorm / rmsnorm

[optimizer]
name = "AdamW"
lr = 3e-4
fused = false
early_step_in_backward = true

[training]
dataset_path = "/scratch/gpfs/mn4560/flash-stu/training/data/fineweb-edu-10B"
num_epochs = 1
microbatch_size = 1
seq_len = 8192
desired_tokens_per_step = 524288 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size ~0.5M tokens
warmup_steps = 1907
max_norm = 1.0
model_dtype = "bfloat16"
steps = 19073
mixed_precision_param = "bfloat16"
mixed_precision_reduce = "float32"
gc_freq = 50
spawn_method = "forkserver"
seed = 1746
deterministic = false

[kernel]
compile = false
compile_cache_size_limit = 8
enable_compiled_autograd = false
matmul_allow_tf32 = false
allow_bf16_reduced_precision_reduction = false
set_float32_matmul_precision = "high"
detect_anomaly = false

# ─────────────────────────────────────────────────────────────────────────────
#  DISTRIBUTED-RELATED CONFIG BLOCKS
# ─────────────────────────────────────────────────────────────────────────────
[distributed.parallelism]
# Data parallel
data_parallel_replicate_degree = 1
data_parallel_shard_degree     = 1
enable_cpu_offload             = false

# Tensor parallel
tensor_parallel_degree         = 1
disable_loss_parallel          = false

# Pipeline parallel
pipeline_parallel_degree       = 1
pipeline_parallel_split_points = []

# TODO: Implement the new DeepSeek pipelining algorithm
pipeline_parallel_schedule     = "1F1B"
# [ "1F1B", "Interleaved1F1B", "GPipe", "LoopedBFS", "InterleavedZeroBubble" ]
# [ "PipelineScheduleSingle", "PipelineScheduleMulti", "ZBVZeroBubble" ]

pipeline_parallel_schedule_csv = ""
# pipeline_parallel_microbatches = 8 # TODO: Use?

# Asynchronous / experimental parallel features
experimental_enable_async_tensor_parallel = true
enable_async_tensor_parallel              = true
context_parallel_degree                   = 1 # NOTE: Should be disabled for Mamba for now.
context_parallel_rotate_method            = "allgather"

[distributed.checkpoint]
enable_checkpoint   = false
folder              = "checkpoint"
interval_type       = "steps"
interval            = 500
model_weights_only  = false
export_dtype        = "float32"
create_seed_checkpoint = false
async_mode          = "disabled"    # ["disabled", "async", "async_with_pinned_mem"]
keep_latest_k       = 0
load_step           = -1

[distributed.activation_checkpoint]
mode               = "full"         # ['none', 'selective', 'full']
selective_ac_option = "2"

[distributed.activation_offloading]
enable                  = false
use_pin_memory         = true
use_streams            = true       # Use null/none to auto-detect based on PyTorch version
max_fwd_stash_size     = 5
min_offload_size       = 1024
virtual_memory_safe_pct = 60.0

[distributed.float8]
enable_float8_linear                    = false # TODO: Fix weird multiple_of 16 thing in Mamba model
enable_fsdp_float8_all_gather           = true
precompute_float8_dynamic_scale_for_fsdp = true

[distributed.comm]
init_timeout_seconds  = 300
train_timeout_seconds = 100
trace_buf_size        = 20000

[memory_estimation]
enabled = false
disable_fake_mode = false
