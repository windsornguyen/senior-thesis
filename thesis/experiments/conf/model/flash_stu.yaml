model_type: FlashSTU

bsz: 1
n_embd: 1536
n_heads: 8
num_layers: 26
seq_len: 8192
window_size: 1024
vocab_size: 200064
mlp_scale: 12
bias: false
dropout: 0.0
num_eigh: 24
use_hankel_L: false
use_flash_fft: true
use_tensordot: true
use_attn: true
softcap: 50.0
dtype: torch.bfloat16

# Derived attributes
hidden_size: ${n_embd}
intermediate_size: ${eval:${n_embd} * ${mlp_scale}}
hidden_act: swish
