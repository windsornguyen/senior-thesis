# -*- coding: utf-8 -*-
"""induction_heads.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfpHgV0A0cVUZksJX5Qpuo7nXxegzgQk

# Effects of Convolutions on Inducing Induction Heads

A study of how local convolutions can help induce the formation of induction heads, potentially without the need of a second attention layer.
"""

# Imports

import time
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import matplotlib

matplotlib.use("Agg")
import matplotlib.pyplot as plt

import os
import pandas as pd  # Added for final summary table

from typing import Any

from tqdm import tqdm

from thesis.experiments.synthetics import registry
from thesis.utils.logger import logger

# Model imports
from fla.models.mamba2.configuration_mamba2 import Mamba2Config
from fla.models.mamba2.modeling_mamba2 import Mamba2ForCausalLM
from fla.models.rwkv7.modeling_rwkv7 import RWKV7ForCausalLM
from fla.models.rwkv7.configuration_rwkv7 import RWKV7Config
from fla.models import DeltaNetConfig, DeltaNetForCausalLM
from fla.models import RetNetConfig, RetNetForCausalLM
from fla.models import GLAConfig, GLAForCausalLM
from fla.models import LinearAttentionConfig, LinearAttentionForCausalLM
from fla.models import TransformerConfig, TransformerForCausalLM
from fla.modules import FusedCrossEntropyLoss

from thesis.models.flash_stu.model import FlashSTU, FlashSTUConfig, get_spectral_filters, nearest_power_of_two

torch.manual_seed(1746)
torch.set_float32_matmul_precision("high")
device = torch.device("cuda")

IGNORE_IDX = -100
SEED = 1746


def apply_compile(model: nn.Module, **kwargs) -> None:
    """
    Apply torch.compile to each main block/layer of the model.
    This attempts to find the list of layers in common attributes like 'layers',
    'blocks', or 'h', making compilation potentially more efficient for models
    with repeated structures.
    """
    layer_attribute_names = ["layers", "blocks", "h"]  # Common names for layer lists
    layer_list = None
    attr_name = None  # Keep track of which attribute name was found

    for current_attr_name in layer_attribute_names:
        if hasattr(model, current_attr_name):
            potential_list = getattr(model, current_attr_name)
            if isinstance(potential_list, (nn.ModuleList, nn.Sequential)):
                # Check if it contains modules (not empty or just parameters)
                if any(isinstance(m, nn.Module) for m in potential_list):
                    layer_list = potential_list
                    attr_name = current_attr_name  # Store the found attribute name
                    logger.info(f"Found layers/blocks for compilation in attribute: '{attr_name}'")
                    break  # Found the layers, stop searching

    if layer_list is None:
        logger.warning(
            f"Could not automatically find a standard layer list ('layers', 'blocks', 'h') "
            f"in {model.__class__.__name__}. Skipping block-wise compilation. "
            f"Consider compiling the whole model if needed."
        )
        return

    logger.info(f"Compiling each block in '{attr_name}' of {model.__class__.__name__} with torch.compile...")
    start = time.perf_counter()
    compiled_layers = nn.ModuleList()  # Create a new list for compiled layers
    for i, block in enumerate(layer_list):
        try:
            # Ensure the block itself is a Module before compiling
            if isinstance(block, nn.Module):
                compiled_block = torch.compile(block, **kwargs)
                compiled_layers.append(compiled_block)
            else:
                logger.warning(f"Item {i} in '{attr_name}' is not an nn.Module, skipping compilation for it.")
                compiled_layers.append(block)  # Keep non-module items as is
        except Exception as e:
            logger.error(
                f"Failed to compile block {i} of {model.__class__.__name__} (from attribute '{attr_name}'): {e}"
            )
            logger.warning(f"Appending original uncompiled block {i} instead.")
            compiled_layers.append(block)  # Add original block if compilation fails

    # Replace the original layer list with the compiled one
    setattr(model, attr_name, compiled_layers)

    end = time.perf_counter()
    logger.info(
        f"Finished processing blocks in '{attr_name}' of {model.__class__.__name__} in {end - start:.4f} seconds."
    )


# Model definitions


# Initialize global registry
class ModelRegistry:
    def __init__(self):
        self.models = {}

    def register(self, name: str, model_cls):
        self.models[name] = model_cls

    def get(self, name: str):
        if name not in self.models:
            raise ValueError(f"Model {name} not found. Available models: {list(self.models.keys())}")
        return self.models[name]

    def list_models(self):
        return list(self.models.keys())


# Register models
MODEL_REGISTRY = ModelRegistry()
# MODEL_REGISTRY.register("flash_stu", FlashSTU)
# MODEL_REGISTRY.register("transformer", TransformerForCausalLM)
MODEL_REGISTRY.register("mamba2", Mamba2ForCausalLM)
MODEL_REGISTRY.register("rwkv7", RWKV7ForCausalLM)
MODEL_REGISTRY.register("deltanet", DeltaNetForCausalLM)
MODEL_REGISTRY.register("retnet", RetNetForCausalLM)
MODEL_REGISTRY.register("gla", GLAForCausalLM)
MODEL_REGISTRY.register("linear_attention", LinearAttentionForCausalLM)


def create_model_config(model_name: str) -> Any:
    """Create appropriate config object for each model type."""
    vocab_size = 8192
    hidden_size = 256
    state_size = 128
    num_hidden_layers = 2
    max_position_embeddings = 512
    initializer_range = 0.006
    norm_eps = 1e-5
    fuse_norm = True
    fuse_swiglu = True
    fuse_cross_entropy = True
    tie_word_embeddings = False
    hidden_act = "swish"

    # Spectron/FlashSTU specific defaults (adjust as needed)
    bsz = 64  # Example batch size
    seq_len = 512  # Example sequence length
    n = nearest_power_of_two(2 * seq_len - 1, round_up=True)
    num_heads = 2  # Example num heads
    dim = hidden_size  # Typically dim matches hidden_size
    head_dim = hidden_size // num_heads
    window_size = seq_len // 8
    mlp_scale = 4  # Common MLP scale
    bias = False  # Often bias is False in transformers
    num_eigh = 32  # Example value
    r = dim  # Example value
    use_hankel_L = False  # Example value
    use_flash_fft = False  # Example value
    use_tensordot = False  # Example value
    use_attn = True  # Assuming FlashSTU specific
    softcap = True  # Assuming FlashSTU specific
    rope_theta = 10000.0  # Assuming FlashSTU specific
    use_alibi = False  # Assuming FlashSTU specific
    torch_dtype = torch.bfloat16

    if model_name == "flash_stu":
        print(device)
        spectral_filters = get_spectral_filters(seq_len, num_eigh, device=device)
        return FlashSTUConfig(
            bsz=bsz,
            dim=dim,
            num_heads=num_heads,
            num_layers=num_hidden_layers,
            seq_len=seq_len,
            n=n,
            weight_tying=tie_word_embeddings,
            window_size=window_size,
            vocab_size=vocab_size,
            inter_dim=hidden_size * mlp_scale,
            mlp_scale=mlp_scale,
            bias=bias,
            num_eigh=num_eigh,
            spectral_filters=spectral_filters,
            r=r,
            use_hankel_L=use_hankel_L,
            use_flash_fft=use_flash_fft,
            use_tensordot=use_tensordot,
            use_attn=use_attn,
            softcap=softcap,
            rope_theta=rope_theta,
            use_alibi=use_alibi,
            torch_dtype=torch_dtype,
        )
    elif model_name == "transformer":
        return TransformerConfig(
            hidden_size=hidden_size,
            num_hidden_layers=num_hidden_layers,
            num_heads=num_heads,
            num_kv_heads=None,
            qkv_bias=False,
            qk_norm=False,
            window_size=None,
            rope_theta=rope_theta,
            max_position_embeddings=max_position_embeddings,
            hidden_ratio=4,
            intermediate_size=None,
            hidden_act=hidden_act,
            initializer_range=initializer_range,
            elementwise_affine=True,
            norm_eps=norm_eps,
            tie_word_embeddings=tie_word_embeddings,
            fuse_norm=fuse_norm,
            fuse_swiglu=fuse_swiglu,
            fuse_cross_entropy=fuse_cross_entropy,
            vocab_size=vocab_size,
        )
    elif model_name == "retnet":
        # Using defaults specified in the original erroneous code
        return RetNetConfig(
            attn_mode="chunk",
            hidden_size=hidden_size,
            expand_k=1,
            expand_v=2,
            hidden_ratio=4,
            intermediate_size=None,
            num_hidden_layers=num_hidden_layers,
            num_heads=2,
            num_kv_heads=None,
            feature_map=None,
            hidden_act=hidden_act,
            use_short_conv=False,
            conv_size=4,
            use_output_gate=True,
            max_position_embeddings=max_position_embeddings,
            elementwise_affine=True,
            norm_eps=norm_eps,
            attn=None,
            tie_word_embeddings=tie_word_embeddings,
            initializer_range=initializer_range,
            fuse_norm=fuse_norm,
            fuse_swiglu=fuse_swiglu,
            fuse_cross_entropy=fuse_cross_entropy,
            vocab_size=vocab_size,
        )
    elif model_name == "gla":
        # Using defaults specified in the original erroneous code
        return GLAConfig(
            hidden_size=hidden_size,
            expand_k=0.5,  # GLA specific default
            expand_v=1,
            hidden_ratio=4,
            intermediate_size=None,
            num_hidden_layers=num_hidden_layers,
            num_heads=2,  # GLA specific default
            num_kv_heads=None,
            feature_map=None,
            attn_mode="chunk",
            use_short_conv=False,
            conv_size=4,
            use_output_gate=True,
            clamp_min=None,
            hidden_act=hidden_act,
            max_position_embeddings=max_position_embeddings,
            elementwise_affine=True,
            norm_eps=norm_eps,
            use_gk=True,
            use_gv=False,
            attn=None,
            tie_word_embeddings=tie_word_embeddings,
            initializer_range=initializer_range,
            fuse_norm=fuse_norm,
            fuse_swiglu=fuse_swiglu,
            fuse_cross_entropy=fuse_cross_entropy,
            vocab_size=vocab_size,
        )
    elif model_name == "linear_attention":
        # Using defaults specified in the original erroneous code
        config = LinearAttentionConfig(
            attn_mode="fused_chunk",
            hidden_size=hidden_size,
            expand_k=1,
            expand_v=1,
            hidden_ratio=4,
            intermediate_size=None,
            num_hidden_layers=num_hidden_layers,
            num_heads=2,  # LinearAttention specific default
            num_kv_heads=None,
            feature_map="elementwise_product",
            tie_feature_map_qk=False,
            norm_q=False,
            norm_k=False,
            norm_feature_map=False,
            hidden_act=hidden_act,
            max_position_embeddings=max_position_embeddings,
            elementwise_affine=True,
            norm_eps=norm_eps,
            attn=None,
            tie_word_embeddings=tie_word_embeddings,
            initializer_range=initializer_range,
            fuse_norm=fuse_norm,
            fuse_swiglu=fuse_swiglu,
            fuse_cross_entropy=fuse_cross_entropy,
            vocab_size=vocab_size,
        )
        # Overrides specified in original code
        config.output_attentions = False
        config.use_cache = False  # Override for training
        config.hidden_act = "swish"  # Explicitly set again
        config.use_lower_bound = False  # Added missing attribute
        return config
    elif model_name == "mamba2":
        # Using defaults specified in the original erroneous code

        return Mamba2Config(
            num_heads=num_heads,  # Mamba specific default
            head_dim=head_dim,  # Mamba specific default
            vocab_size=vocab_size,
            hidden_size=hidden_size * 8,
            state_size=state_size,
            num_hidden_layers=num_hidden_layers,  # Mamba specific default
            layer_norm_epsilon=1e-5,  # Mamba specific default
            expand=2,
            conv_kernel=4,
            n_groups=1,
            use_bias=False,
            use_conv_bias=True,
            hidden_act="silu",  # Mamba specific default
            initializer_range=0.1,  # Mamba specific default
            residual_in_fp32=True,
            time_step_rank="auto",
            time_step_min=0.001,
            time_step_max=0.1,
            time_step_floor=1e-4,
            time_step_limit=(0.0, float("inf")),
            rescale_prenorm_residual=True,
            rms_norm=True,
            chunk_size=256,
            fuse_norm=fuse_norm,
            fuse_cross_entropy=fuse_cross_entropy,
            tie_word_embeddings=tie_word_embeddings,
        )
    elif model_name == "rwkv7":
        # Using defaults specified in the original erroneous code
        return RWKV7Config(
            attn_mode="chunk",
            hidden_size=hidden_size,
            hidden_ratio=4,
            intermediate_size=None,
            num_hidden_layers=num_hidden_layers,
            head_dim=head_dim,  # RWKV specific default
            num_heads=None,
            decay_low_rank_dim=64,  # RWKV specific default
            gate_low_rank_dim=128,  # RWKV specific default
            a_low_rank_dim=64,  # RWKV specific default
            v_low_rank_dim=16,  # RWKV specific default
            hidden_act="sqrelu",  # RWKV specific default
            max_position_embeddings=max_position_embeddings,
            norm_first=True,
            norm_bias=True,
            norm_eps=1e-5,  # RWKV specific default
            attn=None,
            tie_word_embeddings=tie_word_embeddings,
            initializer_range=initializer_range,
            fuse_norm=fuse_norm,
            fuse_cross_entropy=fuse_cross_entropy,
            vocab_size=vocab_size,
            value_dim=None,
        )
    elif model_name == "deltanet":
        # Using defaults specified in the original erroneous code
        return DeltaNetConfig(
            attn_mode="chunk",
            hidden_size=hidden_size,
            expand_k=1,
            expand_v=1,
            use_gate=False,
            use_short_conv=True,
            conv_size=4,
            use_beta=True,
            use_output_norm=True,
            num_heads=num_heads,  # DeltaNet specific default
            qk_norm="l2",
            qk_activation="silu",
            max_position_embeddings=max_position_embeddings,
            hidden_ratio=4,
            intermediate_size=None,
            hidden_act=hidden_act,
            num_hidden_layers=num_hidden_layers,
            norm_eps=norm_eps,
            attn=None,
            tie_word_embeddings=tie_word_embeddings,
            initializer_range=initializer_range,
            fuse_norm=fuse_norm,
            fuse_swiglu=fuse_swiglu,
            fuse_cross_entropy=fuse_cross_entropy,
            vocab_size=vocab_size,
        )
    # Not training Spectron (PyTorch only script)
    # elif model_name == "spectron":
    #     spectral_filters = get_spectral_filters(seq_len, num_eigh)
    #     return SpectronConfig(
    #         spectral_filters=spectral_filters,
    #         spectral_filters_fft=None,
    #         bsz=bsz,
    #         dim=dim,
    #         num_heads=num_heads,
    #         num_local_heads=None,
    #         num_layers=num_hidden_layers,
    #         seq_len=seq_len,
    #         vocab_size=vocab_size,
    #         inter_dim=hidden_size * mlp_scale,
    #         mlp_scale=mlp_scale,
    #         use_tensordot=use_tensordot,
    #         weight_tying=tie_word_embeddings,
    #         bias=bias,
    #         eps=norm_eps,
    #         dtype=jnp.float32,
    #     )
    else:
        raise ValueError(f"Unknown model type: {model_name}")


# Utility functions
def create_lr_lambda(warmup_steps: int, max_steps: int, max_lr: float, min_lr: float):
    def lr_lambda(step: int) -> float:
        if step < warmup_steps:
            lr = min_lr + (max_lr - min_lr) * step / warmup_steps
        else:
            lr = max_lr - (max_lr - min_lr) * (step - warmup_steps) / max(max_steps - warmup_steps, 1)
        return lr / max_lr

    return lr_lambda


def compute_acc(model, loader, device=None):
    model.eval()
    correct_tokens = 0
    total_tokens = 0
    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            output = model(inputs)
            logits_flat = output.logits.view(-1, output.logits.size(-1))
            targets_flat = targets.view(-1)

            # Get predictions for all positions
            predictions_flat = logits_flat.argmax(dim=-1)

            # Create a mask for relevant tokens (where target != ignore_index)
            mask = targets_flat != IGNORE_IDX

            # Calculate accuracy only on relevant tokens
            correct_tokens += (predictions_flat[mask] == targets_flat[mask]).sum().item()
            total_tokens += mask.sum().item()  # Count only the tokens we care about

    model.train()
    # Avoid division by zero if no relevant tokens found in any batch
    if total_tokens == 0:
        return 0.0
    return 100.0 * correct_tokens / total_tokens


# Parameters
E = 640000  # num_train_examples
L = 512  # Sequence length
V = 8192  # Vocab size (base)
batch_size = 64
max_steps = 100000
warmup_steps = max_steps // 10
eval_period = 250
max_lr = 3e-4
min_lr = 3e-5
DELIMS = 3  # Number of delimiter tokens

# Create JAX data iterators directly using the registry
logger.info("Creating datasets...")
mqar = {
    "task_name": "mqar",
    "num_pairs": 32,
    "alpha": 0.1,
    "batch_size": batch_size,
    "num_train": E,
    "num_test": E // 10,
    "backend": "torch",
    "device": device,
    "in_memory": True,  # Use MemoryDataset with caching
    # Add other task-specific kwargs here:
    "vocab_size": V,
    "seq_len": L,
    "num_workers": 0,
}
induction_heads = {
    "task_name": "induction_heads",
    "batch_size": batch_size,
    "num_train": E,
    "num_test": E // 10,
    "backend": "torch",
    "device": device,
    "in_memory": True,  # Use MemoryDataset with caching
    # Task-specific kwargs here:
    "vocab_size": V,
    "seq_len": L,
    "num_workers": 0,
}

in_context_recall = {
    "task_name": "in_context_recall",
    "multi_query": False,
    "batch_size": batch_size,
    "num_train": E,
    "num_test": E // 10,
    "backend": "torch",
    "device": device,
    "in_memory": True,  # Use MemoryDataset with caching
    # Task-specific kwargs here:
    "vocab_size": V,
    "seq_len": L,
    "num_workers": 0,
}
multi_query_in_context_recall = {
    "task_name": "in_context_recall",
    "multi_query": True,
    "batch_size": batch_size,
    "num_train": E,
    "num_test": E // 10,
    "backend": "torch",
    "device": device,
    "in_memory": True,  # Use MemoryDataset with caching
    # Task-specific kwargs here:
    "vocab_size": V,
    "seq_len": L,
    "num_workers": 0,
}
copying = {
    "task_name": "copying",
    "batch_size": batch_size,
    "num_train": E,
    "num_test": E // 10,
    "backend": "torch",
    "device": device,
    "in_memory": True,  # Use MemoryDataset with caching
    # Task-specific kwargs here:
    "vocab_size": V,
    "seq_len": L,
    "num_tokens_to_copy": 16,
    "num_workers": 0,
}
selective_copying = {
    "task_name": "selective_copying",
    "batch_size": batch_size,
    "num_train": E,
    "num_test": E // 10,
    "backend": "torch",
    "device": device,
    "in_memory": True,  # Use MemoryDataset with caching
    # Task-specific kwargs here:
    "vocab_size": V,
    "seq_len": L,
    "num_tokens_to_copy": 16,
    "num_workers": 0,
}
memorization = {
    "vocab_size": 8192,
    "seq_len": 512,
    "kv_map": None,
    "noise_vocab_size": 0,
    "frac_noise": 0,
    "target_ignore_idx": -100,
    "kv_map_seed": 12345,
}

# Set up training components common across models
loss_fn = FusedCrossEntropyLoss(ignore_index=IGNORE_IDX)
baseline_loss = math.log(V)  # Assumes V is the same for all models considered here

# Ensure the registry has models to train
models_to_train = MODEL_REGISTRY.list_models()
logger.info(f"Found models to train: {models_to_train}")

output_dir = "training_plots"
os.makedirs(output_dir, exist_ok=True)
log_dir = "training_logs"
os.makedirs(log_dir, exist_ok=True)

# List to store results for final summary
all_model_results = []

# --- Main Training Loop --- #
for model_name in models_to_train:
    logger.info(f"\n{'=' * 20} Training Model: {model_name.upper()} {'=' * 20}")

    # 1. Create fresh data loaders for the current model
    # Assuming 'copying' task for all models for simplicity, adjust if needed
    logger.info(f"Creating data loaders for task: copying")
    train_loader, val_loader = registry.create_data_loaders(**copying)

    # 2. Initialize model, optimizer, and scheduler
    logger.info(f"Initializing model: {model_name}")
    config = create_model_config(model_name)
    model_cls = MODEL_REGISTRY.get(model_name)
    model = model_cls(config)
    apply_compile(model)
    model.apply(model._init_weights)
    model.to(device=device, dtype=torch.bfloat16)
    logger.info(f"Model initialized with config:\n{config}")
    logger.info(f"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")

    optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)
    lr_lambda_fn = create_lr_lambda(warmup_steps, max_steps, max_lr, min_lr)
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda_fn)
    logger.info("Optimizer and scheduler initialized.")

    # 3. Reset training state variables
    loss_history = []
    acc_history = []
    eval_steps = []
    curr_step = 0
    running_acc = 0.0
    examples_seen = 0
    epochs_completed = 0
    reached_90 = False
    best_val_acc = 0.0

    # >>> Added tracking variables <<<
    steps_to_90 = -1
    max_grad_norm = 0.0
    total_step_time = 0.0
    # >>>------------------------<<<

    # 4. Training loop for the current model
    pbar = tqdm(
        total=max_steps,
        desc=f"Training {model_name}",
        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]",
    )

    model.train()
    # Ensure train_loader is an iterator
    try:
        train_iter = iter(train_loader)
    except TypeError:
        logger.error(f"train_loader for {model_name} is not iterable. Check data loading.")
        continue  # Skip to next model

    start_time = time.time()
    while curr_step < max_steps:
        # >>> Track step time <<<
        step_start_time = time.perf_counter()
        # >>>-----------------<<<
        try:
            inputs, targets = next(train_iter)
        except StopIteration:
            epochs_completed += 1
            logger.info(f"Epoch {epochs_completed} completed for {model_name}. Resetting data loader.")
            # Reinitialize the iterator
            train_iter = iter(train_loader)
            inputs, targets = next(train_iter)
        except Exception as e:
            logger.error(f"Error getting batch for {model_name}: {e}")
            break  # Exit training loop for this model on data error

        batch_examples = inputs.size(0)
        examples_seen += batch_examples

        inputs, targets = inputs.to(device), targets.to(device)

        optimizer.zero_grad()
        output = model(input_ids=inputs, labels=targets)

        # Use the loss returned by the model if available (handles fused CE)
        if output.loss is not None:
            loss = output.loss
        else:
            # Fallback if the model doesn't return loss (e.g., eval or fuse_cross_entropy=False)
            logits = output.logits
            if logits is None:
                logger.error(f"Model {model_name} returned None logits and None loss during training.")
                # Handle error appropriately, maybe break or continue
                break  # Simple break for now
            # Calculate loss manually if not returned by the model
            loss = loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))

        if torch.isnan(loss):
            logger.warning(f"NaN loss detected at step {curr_step} for {model_name}. Stopping training.")
            break  # Stop training this model

        loss.backward()

        # Clip gradients
        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        scheduler.step()

        # >>> Update step time <<<
        step_end_time = time.perf_counter()
        total_step_time += step_end_time - step_start_time
        # >>>------------------<<<

        curr_loss = loss.detach().item()
        loss_history.append(curr_loss)
        curr_step += 1

        # Evaluation - Update metrics periodically
        if curr_step % eval_period == 0 or curr_step == max_steps:
            eval_start_time = time.perf_counter()
            acc = compute_acc(model, val_loader, device=device)
            eval_time = time.perf_counter() - eval_start_time
            acc_history.append(acc)
            eval_steps.append(curr_step)
            running_acc = acc  # Keep using running_acc as it's updated here
            best_val_acc = max(best_val_acc, acc)

            if not reached_90 and acc >= 90.0:
                logger.info(
                    f"{model_name} reached 90% accuracy at step {curr_step}, examples seen: {examples_seen}, epochs: {epochs_completed}"
                )
                reached_90 = True
                steps_to_90 = curr_step  # >>> Store steps_to_90 <<<

        # Update TQDM postfix EVERY step
        pbar.set_postfix(
            loss=f"{curr_loss:.3f}",
            acc=f"{running_acc:.1f}%",  # Display last known accuracy from eval block
            best_acc=f"{best_val_acc:.1f}%",  # Display best accuracy seen so far
            lr=f"{scheduler.get_last_lr()[0]:.1e}",
            gnorm=f"{norm:.2f}",  # Display current step's grad norm (returned by clip_grad_norm_)
            ex=f"{examples_seen // 1000}k",
            ep=f"{epochs_completed}",
        )

        pbar.update(1)

    pbar.close()
    end_time = time.perf_counter()
    total_training_time = end_time - start_time
    # >>> Calculate average step time <<<
    avg_step_time = total_step_time / curr_step if curr_step > 0 else 0
    # >>>---------------------------<<<

    logger.info(f"\nTraining complete for {model_name}!")
    logger.info(f"Total training time: {total_training_time:.2f} seconds")
    logger.info(f"Average step time: {avg_step_time * 1000:.2f} ms")  # Added avg step time
    if loss_history:
        logger.info(f"Final loss: {loss_history[-1]:.4f} (Baseline: {baseline_loss:.4f})")
    if acc_history:
        logger.info(
            f"Final accuracy: {acc_history[-1]:.2f}% (Best: {best_val_acc:.2f}%) (Steps to 90%: {steps_to_90 if steps_to_90 != -1 else 'N/A'})"
        )  # Added steps to 90
    logger.info(f"Max gradient norm observed: {max_grad_norm:.2f}")  # Added max grad norm
    logger.info(f"Total examples seen: {examples_seen}")
    logger.info(f"Epochs completed: {epochs_completed}")

    # >>> Store results for final summary <<<
    model_summary = {
        "Model": model_name,
        "Final Loss": loss_history[-1] if loss_history else float("nan"),
        "Best Acc (%) ": best_val_acc,
        "Final Acc (%) ": acc_history[-1] if acc_history else float("nan"),
        "Steps to 90%": steps_to_90 if steps_to_90 != -1 else "N/A",
        "Total Time (s)": total_training_time,
        "Avg Step Time (ms)": avg_step_time * 1000,
        "Max Grad Norm": max_grad_norm,
        "Total Steps": curr_step,
    }
    all_model_results.append(model_summary)
    # >>>---------------------------------<<<

    # 5. Plotting for the current model
    if loss_history and acc_history:
        plt.figure(figsize=(12, 5))
        plt.suptitle(f"Training Results for {model_name.upper()}")

        plt.subplot(1, 2, 1)
        plt.plot(loss_history, label="Training Loss")
        plt.axhline(y=baseline_loss, color="r", linestyle="--", label="Baseline Loss")
        plt.xlabel("Training Steps")
        plt.ylabel("Loss")
        plt.title("Training Loss Over Steps")
        plt.legend()
        plt.grid(True)

        plt.subplot(1, 2, 2)
        plt.plot(eval_steps, acc_history, marker="o", linestyle="-", color="orange", label="Validation Accuracy")
        plt.axhline(y=best_val_acc, color="g", linestyle=":", label=f"Best Acc ({best_val_acc:.1f}%)")
        plt.xlabel("Training Steps")
        plt.ylabel("Accuracy (%)")
        plt.title("Validation Accuracy Over Steps")
        plt.legend()
        plt.grid(True)

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to prevent title overlap
        plot_filename = os.path.join(output_dir, f"{model_name}_training_plots.png")
        plt.savefig(plot_filename)
        logger.info(f"Saved training plots to {plot_filename}")
        plt.close()  # Close the figure to free memory
    else:
        logger.warning(f"Skipping plotting for {model_name} due to missing data.")

    # 6. Save detailed training log
    if loss_history or acc_history:
        log_filename = os.path.join(log_dir, f"{model_name}_training_log.csv")
        try:
            # Create a DataFrame for detailed logging
            max_steps_recorded = curr_step
            steps = list(range(1, max_steps_recorded + 1))
            log_data = pd.DataFrame({"Step": steps})

            # Add loss history (recorded every step)
            if len(loss_history) == max_steps_recorded:
                log_data["Loss"] = loss_history
            else:
                # Handle potential mismatch if training stopped early
                log_data["Loss"] = loss_history[:max_steps_recorded] + [float("nan")] * (
                    max_steps_recorded - len(loss_history)
                )

            # Add accuracy history (recorded at eval steps)
            if acc_history and eval_steps:
                acc_map = dict(zip(eval_steps, acc_history))
                log_data["Validation_Accuracy"] = log_data["Step"].map(
                    acc_map
                )  # Maps accuracy to corresponding eval step
            else:
                log_data["Validation_Accuracy"] = float("nan")

            log_data.to_csv(log_filename, index=False)
            logger.info(f"Saved detailed training log to {log_filename}")
        except Exception as e:
            logger.error(f"Failed to save detailed log for {model_name}: {e}")
    else:
        logger.warning(f"Skipping detailed logging for {model_name} due to missing data.")

# --- Final Summary --- #
logger.info("\n{'='*20} Final Model Comparison {'='*20}")
if all_model_results:
    results_df = pd.DataFrame(all_model_results)
    results_df = results_df.round(2)  # Round numerical columns
    # Format specific columns
    if "Avg Step Time (ms)" in results_df.columns:
        results_df["Avg Step Time (ms)"] = results_df["Avg Step Time (ms)"].map("{:.2f}".format)
    if "Total Time (s)" in results_df.columns:
        results_df["Total Time (s)"] = results_df["Total Time (s)"].map("{:.2f}".format)
    if "Final Loss" in results_df.columns:
        results_df["Final Loss"] = results_df["Final Loss"].map("{:.4f}".format)
    if "Max Grad Norm" in results_df.columns:
        results_df["Max Grad Norm"] = results_df["Max Grad Norm"].map("{:.2f}".format)

    # Convert DataFrame to string for logging (adjust display options if needed)
    pd.set_option("display.max_rows", None)
    pd.set_option("display.max_columns", None)
    pd.set_option("display.width", 1000)
    pd.set_option("display.colheader_justify", "center")
    pd.set_option("display.precision", 2)

    summary_table = results_df.to_string(index=False)
    logger.info(f"\n{summary_table}")

    # Save summary table to CSV
    summary_log_filename = os.path.join(log_dir, "all_models_summary.csv")
    try:
        results_df.to_csv(summary_log_filename, index=False)
        logger.info(f"Saved summary results table to {summary_log_filename}")
    except Exception as e:
        logger.error(f"Failed to save summary log: {e}")
else:
    logger.info("No models were successfully trained to generate a summary.")

logger.info("\nScript finished.")
