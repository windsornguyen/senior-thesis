# -*- coding: utf-8 -*-
"""induction_heads.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfpHgV0A0cVUZksJX5Qpuo7nXxegzgQk

# Effects of Convolutions on Inducing Induction Heads

A study of how local convolutions can help induce the formation of induction heads, potentially without the need of a second attention layer.
"""

# Imports

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import jax
import jax.numpy as jnp
import numpy as np
import matplotlib.pyplot as plt

from tqdm import tqdm
from torch.utils.data import DataLoader, TensorDataset

torch.manual_seed(1746)
torch.set_float32_matmul_precision("high")

IGNORE_IDX = -1
SEED = 1746

def generate_induction_heads(
    num_examples: int = 5,
    sequence_len: int = 30,
    vocab_size: int = 20,
    seed: int = 0,
) -> TensorDataset:
    torch.manual_seed(seed)
    special = vocab_size - 1
    inputs = torch.randint(0, vocab_size - 1, (num_examples, sequence_len), dtype=torch.long)
    idx = torch.randint(0, sequence_len - 2, (num_examples,), dtype=torch.long)
    inputs[torch.arange(num_examples), idx] = special
    inputs[torch.arange(num_examples), -1] = special
    targets = inputs[torch.arange(num_examples), idx + 1]
    return TensorDataset(inputs, targets)

sample_loader = generate_induction_heads(num_examples=3, sequence_len=24, vocab_size=3)
for batch_ndx, sample in enumerate(sample_loader):
    print(f"Batch {batch_ndx+1}:")
    print(f"Inputs: {sample[0]}")
    print(f"Targets: {sample[1]}")

# Model definitions


class MLP(nn.Module):
    def __init__(self, dim: int, inter_dim: int) -> None:
        super().__init__()
        self.w1 = nn.Linear(dim, inter_dim, bias=False)
        self.w2 = nn.Linear(inter_dim, dim, bias=False)

    def forward(self, x):
        x = self.w1(x)
        x = F.gelu(x, approximate="tanh")
        x = self.w2(x)
        return x


class Attention(nn.Module):
    def __init__(self, dim, num_heads, seq_len) -> None:
        super().__init__()
        self.dim = dim
        self.H = num_heads
        self.h = dim // num_heads
        self.seq_len = seq_len
        self.scale = self.h**-0.5

        # Create causal mask
        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        self.register_buffer("mask", mask.view(1, 1, seq_len, seq_len))

        self.wq = nn.Linear(dim, dim, bias=False)
        self.wk = nn.Linear(dim, dim, bias=False)
        self.wv = nn.Linear(dim, dim, bias=False)
        self.wo = nn.Linear(dim, dim, bias=False)

    def forward(self, x):
        B, L, D = x.shape
        q, k, v = self.wq(x), self.wk(x) * self.scale, self.wv(x)
        q = q.view(B, L, self.H, self.h).transpose(1, 2)
        k = k.view(B, L, self.H, self.h).transpose(1, 2)
        v = v.view(B, L, self.H, self.h).transpose(1, 2)
        sim = q @ k.transpose(-2, -1)

        # Apply causal mask
        sim = sim.masked_fill(self.mask, float("-inf"))

        attn = F.softmax(sim, dim=-1)
        ctxt = attn @ v
        out = ctxt.transpose(1, 2).reshape(B, L, -1)
        out = self.wo(out)
        return out


class AttentionLayer(nn.Module):
    def __init__(self, dim, num_heads, seq_len) -> None:
        super().__init__()
        self.attn = Attention(dim, num_heads, seq_len)
        self.norm = nn.LayerNorm(dim)
        self.mlp = MLP(dim, 4 * dim)
        self.mlp_norm = nn.LayerNorm(dim)

    def forward(self, x):
        x = x + self.attn(self.norm(x))
        # x = x + self.mlp(self.mlp_norm(x))
        return x


class Transformer(nn.Module):
    def __init__(self, dim, num_heads, num_layers, seq_len, vocab_size) -> None:
        super().__init__()
        self.pos_emb = nn.Embedding(seq_len, dim)
        self.tok_emb = nn.Embedding(vocab_size, dim)
        self.layers = nn.ModuleList()
        for _ in range(num_layers):
            attn_layer = AttentionLayer(dim, num_heads, seq_len)
            self.layers.append(attn_layer)
        self.norm_f = nn.LayerNorm(dim)
        self.out = nn.Linear(dim, vocab_size, bias=False)

    def forward(self, x):
        B, L = x.size()
        pos = torch.arange(0, L, dtype=torch.long, device=x.device)
        pos_emb = self.pos_emb(pos)  # [B, L, D]
        tok_emb = self.tok_emb(x)  # [B, L, D]
        x = pos_emb + tok_emb

        for layer in self.layers:
            x = layer(x)

        x = self.norm_f(x)
        out = self.out(x)
        return out


B, L = 1, 4
V = 16

model = Transformer(dim=8, num_heads=2, num_layers=2, seq_len=L, vocab_size=V)
x = torch.randint(0, V, (B, L))
preds = model(x)


# Utility functions
def create_lr_lambda(warmup_steps: int, max_steps: int, max_lr: float, min_lr: float):
    def lr_lambda(step: int) -> float:
        if step < warmup_steps:
            lr = min_lr + (max_lr - min_lr) * step / warmup_steps
        else:
            lr = max_lr - (max_lr - min_lr) * (step - warmup_steps) / max(max_steps - warmup_steps, 1)
        return lr / max_lr

    return lr_lambda


def compute_acc(model, loader, device=None):
    model.eval()
    correct_tokens = 0
    total_tokens = 0
    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            logits = model(inputs)
            predictions = logits[:, -1, :].argmax(dim=-1)
            correct_tokens += (predictions == targets).sum().item()
            total_tokens += targets.size(0)

    model.train()
    return 100.0 * correct_tokens / total_tokens

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Parameters
E = 1000
L = 32
V = 16
dim = 128
num_heads = 2
num_layers = 2
DELIMS = 3

# Create datasets
train_dataset = generate_induction_heads(num_examples=E, sequence_len=L, vocab_size=V, seed=SEED)
val_dataset = generate_induction_heads(num_examples=E // 10, sequence_len=L, vocab_size=V, seed=SEED+1)  # Different seed for val

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Initialize model
model = Transformer(dim=dim, num_heads=num_heads, num_layers=num_layers, seq_len=L, vocab_size=V+DELIMS)
model.to(device)

# Set up training components
max_lr = 1e-3
min_lr = 1e-4
optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)
loss_fn = nn.CrossEntropyLoss(ignore_index=IGNORE_IDX)

max_steps = 10000
warmup_steps = max_steps // 10
eval_period = max_steps // 50

lr_lambda_fn = create_lr_lambda(warmup_steps, max_steps, max_lr, min_lr)
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda_fn)

# Compute baseline loss
total_vocab = V + DELIMS
baseline_loss = math.log(total_vocab)

# Training loop
loss_history = []
acc_history = []
eval_steps = []
curr_step = 0
running_acc = 0.0
examples_seen = 0
epochs_completed = 0
reached_90 = False

pbar = tqdm(total=max_steps, desc="Training", bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]")

model.train()
train_iter = iter(train_loader)

while curr_step < max_steps:
    try:
        inputs, targets = next(train_iter)
    except StopIteration:
        train_iter = iter(train_loader)
        inputs, targets = next(train_iter)
        epochs_completed += 1

    batch_examples = inputs.size(0)
    examples_seen += batch_examples

    inputs, targets = inputs.to(device), targets.to(device)

    optimizer.zero_grad()
    logits = model(inputs)
    last_logits = logits[:, -1, :]
    loss = loss_fn(last_logits, targets)
    loss.backward()
    optimizer.step()
    scheduler.step()

    curr_loss = loss.item()
    loss_history.append(curr_loss)
    curr_step += 1

    if curr_step % eval_period == 0 or curr_step == max_steps:
        acc = compute_acc(model, val_loader, device=device)
        acc_history.append(acc)
        eval_steps.append(curr_step)
        running_acc = acc
        if not reached_90 and acc >= 90.0:
            print(f"Reached 90% accuracy at step {curr_step}, examples seen: {examples_seen}, epochs: {epochs_completed}")
            reached_90 = True

    pbar.set_postfix(
        loss=f"{curr_loss:.3f}",
        base=f"{baseline_loss:.3f}",
        acc=f"{running_acc:.1f}%",
        lr=f"{scheduler.get_last_lr()[0]:.1e}",
        ex=f"{examples_seen//1000}k",
        ep=f"{epochs_completed}"
    )
    pbar.update(1)

pbar.close()
print("\nTraining complete!")
print(f"Final loss: {loss_history[-1]:.4f} (Baseline: {baseline_loss:.4f})")
print(f"Final accuracy: {acc_history[-1]:.2f}%")
print(f"Total examples seen: {examples_seen}")
print(f"Epochs completed: {epochs_completed}")

# Plotting
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(loss_history, label="Training Loss")
plt.axhline(y=baseline_loss, color="r", linestyle="--", label="Baseline Loss")
plt.xlabel("Training Steps")
plt.ylabel("Loss")
plt.title("Training Loss Over Steps")
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(eval_steps, acc_history, marker="o", color="orange", label="Validation Accuracy")
plt.xlabel("Training Steps")
plt.ylabel("Accuracy (%)")
plt.title("Validation Accuracy Over Steps")
plt.legend()
plt.tight_layout()
plt.show()
